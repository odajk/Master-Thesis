\chapter{Discussion and conclusion}
In this thesis, we have performed experiments of statistical inference using subsampling methods for Markov Chain Monte Carlo. As we saw in chapter \ref{chap:method}, the Firefly Monte Carlo, has the correct posterior distribution as its invariant distribution, while the Bardenet et al. 2014 subsampler and Bardenet et al. 2017 subsampler like the naive subsampler sample from an approximation of the correct posterior distribution. The subsamplers suggested by Bardenet, unlike the naive subsampler, ensures that the approximation of the acceptance is controlled, and thus that the approximation of the posterior distribution is controlled.  In this chapter we will discuss the results of the algorithms that were tested in Chapter \ref{chap:experiments}. 



\section{Suggested further work}
\subsection{Quiroz et al. 2019}
\cite{quiroz2019speeding} suggest adding control variates to the naive subsampling method to control the variance of the log-likelihood ratio. They use a Taylor expansion about the parameter space in addition to a Taylor expansion about the nearest centroid in data space. The centroid in data space is calculated using the clustering algorithm given below. 
\todo{Kan ikke kalle data for $z$}
\begin{algorithm}
\caption{Data clustering}
\label{algo:clustering}
\begin{algorithmic}[1]
    \Function{clustering}{$x, y, \epsilon$}
    \State $z_i \gets \left(y_i, x_i\right)^T$
    \State $z \gets \left(z_1^T, \ldots z_n^T\right)^T $
    \State $I \gets \left(0, \ldots, 0\right)^T$ 
    \State $\left(j,k\right) \gets \left(0,0\right)$
    \While{$\sum I_j \neq N$}
    \If{$I_j = 0$}
    \State $C_k \gets \left\{i; \lVert z_j - z_i \rVert \leq \epsilon\right\}  $
    \State $N_k = \left| C_k\right|$ 
    \State $z^{c_k} = \frac{1}{N_k} \sum_{i\in C_k} z_i$
    \State $I_{c_k} \gets 1$
    \State $k\gets k+1$
    \EndIf
    \State $j \gets j+1$
    \EndWhile
    \State $K\gets k$
    \State \textbf{return}$\left\{z^{c_k}\right\}_{k = 1}^K, \; \left\{C_k\right\}_{k=1}^K$
    \EndFunction
    \end{algorithmic}
\end{algorithm}{}
After clustering the data as described by Algorithm \ref{algo:clustering}, \cite{quiroz2019speeding} make a Taylor approximation of the log-likelihood $\ell_i\left(z_i;\theta\right)$ $q\left(z_i, \theta\right)$ about the centroid of the cluster that $z_i$ belongs to.  
\begin{equation}
    q_{i, n}\left(\theta\right) = q\left(z_i;\theta\right) = \ell\left(z^c, \theta\right) + \nabla_z \ell\left(z^c; \theta\right)^T \left(z_i - z^c \right) + \frac{1}{2}\left(z_i - z^c\right)^T H\left(z^c;\theta\right)\left(z_i - z^c\right)
\end{equation}
With $H\left(z^c; \theta\right)$ the Hessian evaluated at $z^c$. 
They define $d_{i, n} = \ell_i\left(\theta\right) - q_{i,n}\left(\theta\right)$ and 
\begin{equation*}
\begin{split}
    \mu_{d,n}\left(\theta\right) &\coloneqq \frac{1}{n}\sum_{i = 1}^n d_{i,n}\left(\theta\right) \\
    \sigma_{d,n} &\coloneqq \frac{\sum_{i=1}^n\left(d_{i,n}\left(\theta\right) - \mu_{d,n}\left(\theta\right)\right)^2}{n}
\end{split}
\end{equation*}{}
the mean and variance of $\left\{d_{i,n}\left(\theta\right)\right\}_{i=1}^k$. Next, they let $u_1, \ldots, u_m$ be iid random variables with $Pr\left(u = k\right) = \frac{1}{n}$ for $k = 1, \ldots n$, i.e. $u_1, \ldots, u_m$ is a $m$-sized sample of the numbers between $1$ and $n$ with equal probability of each number. \todo{with replacement, right?} 
Then they define the \textbf{difference estimator} of $\ell_n\left(\theta\right)$
\begin{equation}
    \hat{\ell}_{\left(m,n\right)}\coloneqq q_{\left(n\right)}\left(\theta\right) + n\hat{\mu}_{d,n}\left(\theta\right) 
\end{equation}
where 
\begin{equation*}
    \hat{\mu}_{d,n}\left(\theta\right) = \frac{1}{m}\sum_{i=1}^m d_{u_i,n}\left(\theta\right)
\end{equation*}{}
and 
\begin{equation*}
    q_{\left(n\right)} \coloneqq \sum_{i=1}^n q_{i,n}\left(\theta\right).
\end{equation*}
\todo{forst√• hvorfor de presenterer Lemma 1}
They also suggest an estimate of $\sigma_{d,n}^2\left(\theta\right)$
\begin{equation}
    \hat{\sigma}_{d,n}^2\left(\theta\right) \coloneqq \frac{\sum_{i = 1}^m \left(d_{u_i, n}\left(\theta\right) - \hat{\mu}_{d,n}\left(\theta\right)\right)^2}{m} 
\end{equation}

\todo{rewrite following}
They propose using an unbiased estimator $\ell_{m,n}\left(\theta\right)$ of the log-likelihood, and then bias-correcting it to obtain the approximately bias-corrected likelihood estimator
\begin{equation}
   \hat{L}_{m,n}\left(\theta, u\right) \coloneqq \exp\left(\hat{\ell}\left(\theta\right) - \frac{n^2}{2m}\hat{\sigma}_{d,n}^2\left(\theta\right)\right)
\end{equation}
with $p_{\Theta}\left(\theta\right)$ as the prior, and the likelihood $L_{\left(n\right)}\left(\theta\right)$, the \textbf{marginal likelihood} is given by $\overline{L}_{\left(n\right)}\left(\theta\right) = \int_{\Theta} L_{\left(n\right)}\left(\theta\right)p_{\Theta}\left(\theta\right) d\theta$. 
Then the posterior $\overline{\pi}_n\left(\theta\right) = L_{\left(n\right)}p_{\Theta}\left(\theta\right)/\overline{L}_{\left(n\right)}\left(\theta\right)$. 
They define $p_U\left(u\right)$ as the distribution of the vector auxiliary variables $u$. 
Then $\hat{L}_{\left(m,n\right)}\left(\theta\right)$, a possibly biased estimator of $L_{\left(n\right)}\left(\theta\right)$, has expectation 
\begin{equation}
    L_{\left(m,n\right)}\left(\theta\right) = \int_U \hat{L}_{\left(m,n\right)}\left(\theta, u\right) p_U\left(u\right) du 
\end{equation}
and 
\begin{equation}
    \overline{\pi}_{\left(m,n\right)} \left(\theta, u\right) \coloneqq \hat{L}_{\left(m,n\right)}\left(\theta, u\right)p_{\Theta}\left(\theta\right)p_U\left(u\right)/\overline{L}_{\left(m,n\right)}.
\end{equation}
With proposal distribution
\begin{equation}
    q_{\Theta, u}\left(\theta', u'\mid \theta, u\right) = p_U\left(u\right)q_{\Theta}\left(\theta'\mid \theta\right)
\end{equation}
Then the resulting acceptance probability $\alpha$ is given by 
\begin{equation}
    \alpha = \min\left(1, \frac{\hat{L}_{\left(m,n\right)}\left(\theta', u'\right)p_{\Theta}\left(\theta'\right)p_U\left(u'\right)q_{\Theta}\left(\theta\mid \theta'\right)}{\hat{L}_{\left(m,n\right)}\left(\theta, u\right)p_{\Theta}\left(\theta\right)p_U\left(u\right)q_{\Theta}\left(\theta'\mid \theta\right)}\right)
\end{equation}
since the propability $p_U\left(u = k\right) = 1/n \forall u$, the terms $p_U\left(u'\right)$ and $p_U\left(u\right)$ cancel, and we get 
\begin{equation}\label{eq:accept_prob_quiroz}
      \alpha = \min\left(1, \frac{\hat{L}_{\left(m,n\right)}\left(\theta', u'\right)p_{\Theta}\left(\theta'\right)q_{\Theta}\left(\theta\mid \theta'\right)}{\hat{L}_{\left(m,n\right)}\left(\theta, u\right)p_{\Theta}\left(\theta\right)q_{\Theta}\left(\theta'\mid \theta\right)}\right)
\end{equation}
\cite{quiroz2019speeding} points out that an MH-sampler with \eqref{eq:accept_prob_quiroz} as acceptance probability, will have stationary distribution $\overline{\pi}_{\left(m,n\right)}\left(\theta\right)$ as stationary distribution, which is equal to $\pi_{\left(n\right)}\left(\theta\right)$ if $\hat{L}_{\left(m,n\right)}\left(\theta, u\right)$ is an unbiased estimator of $L_{\left(n\right)}\left(\theta\right)$.